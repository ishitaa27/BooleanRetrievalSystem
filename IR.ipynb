{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ishitaa27/BooleanRetrievalSystem/blob/main/IR.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-sdDK-u9gsG1",
        "outputId": "8785a5c0-dcc3-4c3c-db79-5dacc127d856"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import io\n",
        "\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "\n",
        "import os,glob\n",
        "import string\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p6nbf75RjX-J",
        "outputId": "620f1d68-b1b8-46ad-80f1-dace7d2a999a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "#mounting drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T_1xChGijEQc"
      },
      "outputs": [],
      "source": [
        "#removing stop words\n",
        "\n",
        "stop_words=nltk.corpus.stopwords.words('english')\n",
        "\n",
        "\n",
        "def preprocess(file1):\n",
        "  line = file1.read()\n",
        "  words_r = line.split()\n",
        "  \n",
        "  # remove punctuation from each word\n",
        "  table = str.maketrans('', '', string.punctuation)\n",
        "  stripped = [w.translate(table) for w in words_r]\n",
        "\n",
        "  words = [word.lower() for word in stripped]\n",
        "  #words=nltk.word_tokenize(stri)\n",
        "\n",
        "  #print(len(words))\n",
        "  #print(words)\n",
        "  final_words=[]\n",
        "  for r in words:\n",
        "      if not r in stop_words:\n",
        "        final_words.append(r)\n",
        "\n",
        "  #print(len(final_words))\n",
        "  #print(final_words)\n",
        "  return final_words\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m8fkX15LnjqQ"
      },
      "outputs": [],
      "source": [
        "#Lemmatization\n",
        "def lemmatization(final_words):\n",
        "  \n",
        "  #an instance of Word Net Lemmatizer\n",
        "  lemmatizer = WordNetLemmatizer()   \n",
        "\n",
        "  lemmatized_words = [lemmatizer.lemmatize(word) for word in final_words]  \n",
        "  unique_words=set(lemmatized_words)\n",
        "  return unique_words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XqAkIoa7o3o1"
      },
      "outputs": [],
      "source": [
        "#generating an inverted index\n",
        "inverted_index={}\n",
        "#set of all documents, required for processing boolean queries\n",
        "universal = []\n",
        "\n",
        "#words is the list of words in the text file after preprocesing\n",
        "#doc is the name of the text file\n",
        "def generate_inverted_index(words,doc):\n",
        "  if doc not in universal:\n",
        "    universal.append(doc)\n",
        "  for word in words:\n",
        "    if word not in inverted_index.keys():\n",
        "      inverted_index[word]=[doc]\n",
        "    elif doc not in inverted_index[word]:\n",
        "      inverted_index[word].append(doc)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6MBqXKHrFWFH"
      },
      "outputs": [],
      "source": [
        "#function that removes the IO wrapper and returns just the path of the file along with the name\n",
        "def extractPath(winnie):\n",
        "    piggy = str(winnie)\n",
        "\n",
        "    foff1 =\"<_io.TextIOWrapper name='\"\n",
        "    foff2 =\"' mode='r' encoding='UTF-8'>\"\n",
        "\n",
        "    piggy = piggy.replace(foff1, \"\")\n",
        "    piggy = piggy.replace(foff2, \"\")\n",
        "\n",
        "    return piggy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LFK_gqUTWObE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5ec2a845-6a53-409e-80c1-60e6c58036cc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time for preprocessing = 5.08 seconds\n",
            "time for creating index after preprocessing = 0.13 seconds\n",
            "time for indexing including the preprocessing = 5.27 seconds\n"
          ]
        }
      ],
      "source": [
        "\n",
        "#Folder path is the root directory which contains the documents needed for processing\n",
        "folder_path='/content/drive/MyDrive/Assignment'\n",
        "func_start_time = time.time()\n",
        "preprocess_time = 0\n",
        "inverted_index_time=0\n",
        "for filename in glob.glob(os.path.join(folder_path,'*.txt')):\n",
        "  with open(filename,'r') as f:\n",
        "    start_time = time.time()\n",
        "    #sending the files for preprocessing\n",
        "    tokens=preprocess(f)\n",
        "    #cleaned tokens are sent for lemmatization\n",
        "    final_tokens=lemmatization(tokens)\n",
        "    preprocess_time += time.time() - start_time\n",
        "    #to extract the path of the file\n",
        "    file_n=extractPath(f)\n",
        "    start_time2=time.time()\n",
        "    #generates inverted index taking name of the document as a parameter and tokens as another\n",
        "    generate_inverted_index(final_tokens,file_n.split('/')[-1])\n",
        "    inverted_index_time+=time.time()-start_time2\n",
        "\n",
        "\n",
        "print(\"time for preprocessing = %.2f seconds\" % (preprocess_time))\n",
        "print(\"time for creating index after preprocessing = %.2f seconds\" % (inverted_index_time))\n",
        "print(\"time for indexing including the preprocessing = %.2f seconds\" % (time.time() - func_start_time))\n",
        "\n",
        "\n",
        "complete_words=list(inverted_index.keys())\n",
        "\n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vXh4taT2qkDK"
      },
      "outputs": [],
      "source": [
        "#returns the word that has least edit distance with given word\n",
        "#if multiple words have same least edit distance, the word occurring in more number of documents is returned\n",
        "def spell_correction(tokens,word):\n",
        "  edit_dist=10000000\n",
        "  for token in tokens:\n",
        "    dist=nltk.edit_distance(token,word)\n",
        "    if dist<edit_dist:\n",
        "      ans=token\n",
        "      edit_dist=dist\n",
        "    elif dist==edit_dist:\n",
        "      if len(inverted_index[token])>len(inverted_index[ans]):\n",
        "        ans=token\n",
        "  return ans"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FMQS8Pttzk43"
      },
      "outputs": [],
      "source": [
        "#Function to find set of documents present in both list1 and list2\n",
        "def find_and(list1,list2):\n",
        "  result=list(set(list1) & set(list2))\n",
        "  return result\n",
        "\n",
        "#function to find list of documents not present in list1\n",
        "def find_not(list1):\n",
        "  result=list(set(universal) - set(list1))\n",
        "  return result\n",
        "\n",
        "#function to find list of documents present in either list1 or list2\n",
        "def find_or(list1,list2):\n",
        "  list1.extend(list2)\n",
        "  result=list(set(list1))\n",
        "  return result\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ElTiOvH_4jhN"
      },
      "outputs": [],
      "source": [
        "#function that returns all the cyclic permutations of given string\n",
        "def permute(input):\n",
        "    tmp=input\n",
        "    s=len(input)+1\n",
        "    input+='$'+input\n",
        "    res=[]\n",
        "    for x in range(s):\n",
        "        #print(input[x:x+s])\n",
        "        res.append(input[x:x+s])\n",
        "     \n",
        "    return res"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gDTEk_nL5rsT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "58c230b3-78b1-432f-aa5b-d78f39ca1e89"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "228654\n"
          ]
        }
      ],
      "source": [
        "#contains all the cyclic permutations of all the words mapped to the unrotated word\n",
        "permuterm_index={}\n",
        "\n",
        "#creates permuterm index for all the words in the list represented by tokens\n",
        "def permuterm(tokens):\n",
        "  for token in tokens:\n",
        "    res=permute(token)\n",
        "    for item in res:\n",
        "      permuterm_index[item]=token\n",
        "\n",
        "permuterm(complete_words)\n",
        "\n",
        "print(len(permuterm_index))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Function that rotates the query for smooth searching\n",
        "#Appending the '$' sign at the end and rotating it so the '*' is at the end for searching\n",
        "def rotate_query(query):\n",
        "  query+='$'\n",
        "  length=len(query)\n",
        "  for d in range(length):\n",
        "    Rfirst = query[0 : len(query)-d]\n",
        "    Rsecond = query[len(query)-d : ]\n",
        "    #creating a permutation by rotating right each time\n",
        "    rotated_query=Rsecond+Rfirst\n",
        "    #returning only the rotation that has '*' as last character\n",
        "    if rotated_query[-1]=='*':\n",
        "      res=rotated_query\n",
        "      break\n",
        "  #return rotated query after stripping the '*'\n",
        "  return res[:-1]"
      ],
      "metadata": {
        "id": "mVHvCUhhqnIO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Does prefix match for given query against all the words in permuterm_index\n",
        "#returns those words whose prefix matches with the query\n",
        "def prefix_match(query):\n",
        "  res=[]\n",
        "  for token in permuterm_index:\n",
        "    #checks whether the token starts with the query\n",
        "    if token.startswith(query):\n",
        "      #print(token)\n",
        "      res.append(permuterm_index[token])\n",
        "  #set contains unique words which satisfy the wildcard \n",
        "  final_res=set(res)\n",
        "  #print(res)\n",
        "  return final_res\n"
      ],
      "metadata": {
        "id": "KhLNvQ-3hfRl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Handles the wildcard queries\n",
        "def wildcard_query(query):\n",
        "  #to rotate the query to suit matching\n",
        "  query=rotate_query(query)\n",
        "  #list containing words satisfying wildcard\n",
        "  matched_words=prefix_match(query)\n",
        "  #docs contains list of documents containing these words\n",
        "  docs=[]\n",
        "  for token in matched_words: \n",
        "    for doc in inverted_index[token]:\n",
        "      docs.append(doc)\n",
        "  \n",
        "  #set containing the unique documents that contain the words satisfying wildcard queries\n",
        "  final_docs=set(docs)\n",
        "  return final_docs\n"
      ],
      "metadata": {
        "id": "GthE3_FRlDHd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2lpNhsp3IDkm"
      },
      "outputs": [],
      "source": [
        "#Function to compute boolean queries\n",
        "def boolean_query(query):\n",
        "  #query=input(\"Enter your query:\")\n",
        "  #converts query to lowercase\n",
        "  query = query.lower()\n",
        "  #stores each argument of query in a list\n",
        "  args = list(query.split(' '))\n",
        "\n",
        "  bool_q = ['and', 'or', 'not']\n",
        "  q_size = len(args)\n",
        "\n",
        "  #Stores all the wildcard terms\n",
        "  wildcard=[]\n",
        "\n",
        "  #To find the wildcard terms\n",
        "  for arg in args:\n",
        "    if \"*\" in arg:\n",
        "      wildcard.append(arg)\n",
        "  \n",
        "  #spell correction for wrongly spelt words\n",
        "  for i in range(len(args)):\n",
        "    if (args[i] not in bool_q) and (args[i] not in complete_words) and (args[i] not in wildcard):\n",
        "      args[i]=spell_correction(complete_words,args[i])\n",
        "\n",
        "  #Initializing ans and checking if query is valid\n",
        "  if args[0]=='not' and (q_size!=1) and (args[1] not in bool_q) and (args[1] not in wildcard):\n",
        "    ans = find_not(inverted_index[args[1]])\n",
        "    i=2\n",
        "  elif args[0]=='not' and q_size!=1 and (args[1] not in bool_q) and (args[1] in wildcard):\n",
        "    ans=find_not(wildcard_query(args[1]))\n",
        "    i=2\n",
        "  elif (args[0] not in bool_q) and (args[0] not in wildcard):\n",
        "    ans = inverted_index[args[0]]\n",
        "    i=1\n",
        "  elif (args[0] not in bool_q) and (args[0] in wildcard):\n",
        "    ans=wildcard_query(args[0])\n",
        "    i=1\n",
        "  else:\n",
        "    print(\"Invalid Query\")\n",
        "    return []\n",
        "  \n",
        "  #Loop to find solutions to all queries\n",
        "  while i<q_size-1:\n",
        "    #Query of form NOT A\n",
        "    if args[i]=='not' and (args[i+1] not in bool_q) and (args[i+1] not in wildcard):\n",
        "      ans = find_not(inverted_index[args[i+1]])\n",
        "      i+=2\n",
        "    #Query of form NOT A*\n",
        "    elif args[i]=='not' and (args[i+1] not in bool_q) and (args[i+1] in wildcard):\n",
        "      ans=find_not(wildcard_query(args[i+1]))\n",
        "      i+=2\n",
        "    #Query of form A AND B\n",
        "    elif args[i]=='and' and args[i+1]!='not' and (args[i+1] not in wildcard):\n",
        "      ans = find_and(ans,inverted_index[args[i+1]])\n",
        "      i+=2\n",
        "    #Query of the form A AND B*\n",
        "    elif args[i]=='and' and args[i+1]!='not' and (args[i+1] in wildcard):\n",
        "      ans=find_and(ans,wildcard_query(args[i+1]))\n",
        "      i+=2\n",
        "    #Query of form A AND NOT B\n",
        "    elif args[i]=='and' and (i<q_size-2) and (args[i+2] not in wildcard):\n",
        "      list1 = find_not(inverted_index[args[i+2]])\n",
        "      ans = find_and(ans,list1)\n",
        "      i+=3\n",
        "    #Query of the form A AND NOT B*\n",
        "    elif args[i]=='and' and (i<q_size-2) and (args[i+2] in wildcard):\n",
        "      list1=find_not(wildcard_query(args[i+2]))\n",
        "      ans = find_and(ans,list1)\n",
        "      i+=3\n",
        "    #Query of form A OR B\n",
        "    elif args[i]=='or' and (args[i+1]!='not') and (args[i+1] not in wildcard):\n",
        "      ans = find_or(ans, inverted_index[args[i+1]])\n",
        "      i+=2\n",
        "    #Query of the form A OR B*\n",
        "    elif args[i]=='or' and (args[i+1]!='not') and (args[i+1] in wildcard):\n",
        "      ans= find_or(ans, wildcard_query(args[i+1]))\n",
        "      i+=2\n",
        "    #Query of form A OR NOT B\n",
        "    elif args[i]=='or' and (i<q_size-2) and (args[i+2] not in wildcard):\n",
        "      list1 = find_not(inverted_index[args[i+2]])\n",
        "      ans = find_or(ans,list1)\n",
        "      i+=3\n",
        "    #Query of the form A OR NOT B*\n",
        "    elif args[i]=='or' and (i<q_size-2) and (args[i+2] in wildcard):\n",
        "      list1=find_not(wildcard_query(args[i+2]))\n",
        "      ans=find_or(ans,list1)\n",
        "      i+=3\n",
        "    #Query is in invalid format\n",
        "    else:\n",
        "      print(\"Invalid Query\")\n",
        "      ans = []\n",
        "      break\n",
        "    #print(i)\n",
        "  return ans\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"You have 3 choices.\")\n",
        "print(\"1.Boolean Queries that prints the documents satisfying the query. It also corrects the spelling if a word is spelt wrong.\")\n",
        "print(\"2.finding the word that is closest to the list of words present in the documents\")\n",
        "num=input(\"Enter your choice\")\n",
        "\n",
        "if num==\"1\":\n",
        "  user_query=input(\"Enter your query:\")\n",
        "  query_start=time.time()\n",
        "  results=boolean_query(user_query)\n",
        "  query_end=time.time()\n",
        "  if len(results)>0:\n",
        "    print(\"Number of documents satisfying the query:- \" ,len(results))\n",
        "    print(\"The Documents:-\")\n",
        "    print(results)\n",
        "    print(\"time for querying = %.2f seconds\" % (query_end - query_start))\n",
        "  else:\n",
        "    print(\"No document satisfies given query\")\n",
        "elif num==\"2\":\n",
        "  user_query=input(\"Enter the word\")\n",
        "  query_start=time.time()\n",
        "  if user_query not in complete_words:\n",
        "    result=spell_correction(complete_words,user_query)\n",
        "    print(\"The closest word is:-\")\n",
        "    print(result)\n",
        "  else:\n",
        "    print(user_query)\n",
        "  query_end=time.time()\n",
        "  print(\"time for querying = %.2f seconds\" % (query_end - query_start))\n"
      ],
      "metadata": {
        "id": "bO8e0Hi4wwwV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b50fca8c-2c5e-407f-8f2d-1c51c63613e6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "You have 3 choices.\n",
            "1.Boolean Queries that prints the documents satisfying the query. It also corrects the spelling if a word is spelt wrong.\n",
            "2.finding the word that is closest to the list of words present in the documents\n",
            "Enter your choice2\n",
            "Enter the wordbillia\n",
            "The closest word is:-\n",
            "william\n",
            "time for querying = 1.45 seconds\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "IR.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}